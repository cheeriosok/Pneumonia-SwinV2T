{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "# import SwinTransformerV2  \n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from SwinTransformer import SwinTransformer\n",
    "from SwinTransformerV2 import SwinTransformerV2\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from engine import train\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "seed = 42\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def set_seeds(seed_value=seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "device = torch.cuda.is_available() and \"cuda\" or \"cpu\"\n",
    "torch.cuda.manual_seed(seed) or torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration for data processing\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 256\n",
    "IMG_SIZE = 256\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomApply([\n",
    "        # Potentially remove or reduce the extent of ColorJitter if dealing with grayscale images\n",
    "        transforms.RandomAffine(degrees=5, translate=(0.02, 0.02), scale=(0.95, 1.05)),\n",
    "        transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2)),  # Reduce the maximum sigma\n",
    "    ], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Adjust if not using pre-trained models or if images are grayscale\n",
    "])\n",
    "current_directory = os.getcwd()\n",
    "training_directory = os.path.join(current_directory, \"Dataset\", \"train\")\n",
    "validation_directory = os.path.join(current_directory, \"Dataset\", \"test\")\n",
    "testing_directory = os.path.join(current_directory, \"Dataset\", \"val\")\n",
    "\n",
    "\n",
    "def setup_dataloaders(train_path, test_path, val_path, transform, batch_size, workers=NUM_WORKERS):\n",
    "    # Load datasets\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(val_path, transform=transform)  \n",
    "    \n",
    "    # Calculate class weights (inverse of the class distribution)\n",
    "    class_sample_counts = [len(np.where(np.array(train_dataset.targets) == i)[0]) for i in np.unique(train_dataset.targets)]\n",
    "    class_weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "    \n",
    "    # Compute sample weights\n",
    "    sample_weights = class_weights[train_dataset.targets]\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader, val_loader, class_weights\n",
    "\n",
    "train_loader, test_loader, val_loader, class_weights = setup_dataloaders(training_directory, testing_directory, validation_directory, transformations, BATCH_SIZE, NUM_WORKERS)\n",
    "class_weights = class_weights.to(device)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    'embed_dim': 96,\n",
    "    'depths': [ 2, 2, 6, 2 ],\n",
    "    'num_heads': [ 2, 4, 8, 16 ],\n",
    "    'window_size': 8,\n",
    "    'ape': False, \n",
    "    'patch_norm': True,  \n",
    "    'num_classes': len(class_weights),\n",
    "    'drop_path_rate': 0.5,\n",
    "}\n",
    "\n",
    "# Model, optimizer, and loss function setup\n",
    "model = SwinTransformerV2(\n",
    "    img_size=IMG_SIZE, \n",
    "    patch_size=32, \n",
    "    in_chans=3, \n",
    "    **model_config\n",
    ")\n",
    "model.to('cuda')\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-06, betas=(0.9, 0.999), weight_decay=1e-2)\n",
    "loss_function =nn.CrossEntropyLoss(weight=class_weights)\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "  Embedding Dimension: 96\n",
      "  Depths: [2, 2, 6, 2]\n",
      "  Number of Heads: [2, 4, 8, 16]\n",
      "  Window Size: 8\n",
      "  Absolute Position Encoding: False\n",
      "  Patch Normalization: True\n",
      "  Number of Classes: 2\n",
      "  Drop Path Rate: 0.5\n",
      "\n",
      "Optimizer and Loss Function:\n",
      "  Learning Rate: 1e-06\n",
      "  Betas: (0.9, 0.999)\n",
      "  Weight Decay: 0.01\n",
      "  Loss Function: CrossEntropyLoss()\n",
      "\n",
      "Training Parameters:\n",
      "  Batch Size: 256\n",
      "  Image Size: 256\n",
      "  Device: cuda\n",
      "  Number of Epochs: 30\n",
      "  Learning Rate Scheduler Step Size: 1\n",
      "  Learning Rate Scheduler Gamma: 1\n"
     ]
    }
   ],
   "source": [
    "# Print model configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Embedding Dimension: {model_config['embed_dim']}\")\n",
    "print(f\"  Depths: {model_config['depths']}\")\n",
    "print(f\"  Number of Heads: {model_config['num_heads']}\")\n",
    "print(f\"  Window Size: {model_config['window_size']}\")\n",
    "print(f\"  Absolute Position Encoding: {model_config['ape']}\")\n",
    "print(f\"  Patch Normalization: {model_config['patch_norm']}\")\n",
    "print(f\"  Number of Classes: {model_config['num_classes']}\")\n",
    "print(f\"  Drop Path Rate: {model_config['drop_path_rate']}\")\n",
    "\n",
    "# Print optimizer and loss function details\n",
    "print(\"\\nOptimizer and Loss Function:\")\n",
    "print(f\"  Learning Rate: {optimizer.defaults['lr']}\")\n",
    "print(f\"  Betas: {optimizer.defaults['betas']}\")\n",
    "print(f\"  Weight Decay: {optimizer.defaults['weight_decay']}\")\n",
    "print(f\"  Loss Function: {loss_function}\")\n",
    "\n",
    "# Print other training parameters\n",
    "print(\"\\nTraining Parameters:\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Image Size: {IMG_SIZE}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Number of Epochs: {30}\")\n",
    "print(f\"  Learning Rate Scheduler Step Size: {1}\")\n",
    "print(f\"  Learning Rate Scheduler Gamma: {1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6d21ea6084febac8188a528ec6369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.658464 | train_acc: 0.471540 | val_loss: 1.213098 | val_acc: 0.319754 | lr: 0.00000100\n",
      "Epoch: 2 | train_loss: 0.511008 | train_acc: 0.566158 | val_loss: 0.842121 | val_acc: 0.471168 | lr: 0.00000100\n",
      "Epoch: 3 | train_loss: 0.434949 | train_acc: 0.647321 | val_loss: 0.748352 | val_acc: 0.597098 | lr: 0.00000100\n",
      "Epoch: 4 | train_loss: 0.373820 | train_acc: 0.706411 | val_loss: 0.640981 | val_acc: 0.670573 | lr: 0.00000100\n",
      "Epoch: 5 | train_loss: 0.332641 | train_acc: 0.762897 | val_loss: 0.558566 | val_acc: 0.736607 | lr: 0.00000100\n",
      "Epoch: 6 | train_loss: 0.310131 | train_acc: 0.787760 | val_loss: 0.526036 | val_acc: 0.736979 | lr: 0.00000100\n",
      "Epoch: 7 | train_loss: 0.279436 | train_acc: 0.815724 | val_loss: 0.485411 | val_acc: 0.760975 | lr: 0.00000100\n",
      "Epoch: 8 | train_loss: 0.263184 | train_acc: 0.834635 | val_loss: 0.486995 | val_acc: 0.759673 | lr: 0.00000100\n",
      "Epoch: 9 | train_loss: 0.252353 | train_acc: 0.841580 | val_loss: 0.452744 | val_acc: 0.787760 | lr: 0.00000100\n",
      "Epoch: 10 | train_loss: 0.243718 | train_acc: 0.854725 | val_loss: 0.435229 | val_acc: 0.822359 | lr: 0.00000100\n",
      "Epoch: 11 | train_loss: 0.224149 | train_acc: 0.871900 | val_loss: 0.405211 | val_acc: 0.829613 | lr: 0.00000100\n",
      "Epoch: 12 | train_loss: 0.206025 | train_acc: 0.878162 | val_loss: 0.437871 | val_acc: 0.810082 | lr: 0.00000100\n",
      "Epoch: 13 | train_loss: 0.196171 | train_acc: 0.887401 | val_loss: 0.405618 | val_acc: 0.825707 | lr: 0.00000100\n",
      "Epoch: 14 | train_loss: 0.190114 | train_acc: 0.895089 | val_loss: 0.432887 | val_acc: 0.818266 | lr: 0.00000100\n",
      "Epoch: 15 | train_loss: 0.176067 | train_acc: 0.902158 | val_loss: 0.413758 | val_acc: 0.823475 | lr: 0.00000100\n",
      "Epoch: 16 | train_loss: 0.184269 | train_acc: 0.888393 | val_loss: 0.402322 | val_acc: 0.830729 | lr: 0.00000100\n",
      "Epoch: 17 | train_loss: 0.171889 | train_acc: 0.904886 | val_loss: 0.431907 | val_acc: 0.824219 | lr: 0.00000100\n",
      "Epoch: 18 | train_loss: 0.178078 | train_acc: 0.898748 | val_loss: 0.431624 | val_acc: 0.828497 | lr: 0.00000100\n",
      "Epoch: 19 | train_loss: 0.164385 | train_acc: 0.909040 | val_loss: 0.441365 | val_acc: 0.829799 | lr: 0.00000100\n",
      "Epoch: 20 | train_loss: 0.158487 | train_acc: 0.912760 | val_loss: 0.427060 | val_acc: 0.830729 | lr: 0.00000100\n",
      "Epoch: 21 | train_loss: 0.153747 | train_acc: 0.916915 | val_loss: 0.456757 | val_acc: 0.811756 | lr: 0.00000100\n",
      "Epoch: 22 | train_loss: 0.159078 | train_acc: 0.913008 | val_loss: 0.455332 | val_acc: 0.814174 | lr: 0.00000100\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 23 | train_loss: 0.155827 | train_acc: 0.921875 | val_loss: 0.454591 | val_acc: 0.828125 | lr: 0.00000010\n",
      "Epoch: 24 | train_loss: 0.165102 | train_acc: 0.911334 | val_loss: 0.458545 | val_acc: 0.820685 | lr: 0.00000010\n",
      "Epoch: 25 | train_loss: 0.150349 | train_acc: 0.915365 | val_loss: 0.451405 | val_acc: 0.835751 | lr: 0.00000010\n",
      "Epoch: 26 | train_loss: 0.155043 | train_acc: 0.917597 | val_loss: 0.449801 | val_acc: 0.820312 | lr: 0.00000010\n",
      "Epoch: 27 | train_loss: 0.151180 | train_acc: 0.913938 | val_loss: 0.472542 | val_acc: 0.820685 | lr: 0.00000010\n",
      "Epoch: 28 | train_loss: 0.150512 | train_acc: 0.915489 | val_loss: 0.443284 | val_acc: 0.827195 | lr: 0.00000010\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 29 | train_loss: 0.154057 | train_acc: 0.912946 | val_loss: 0.454844 | val_acc: 0.827567 | lr: 0.00000001\n",
      "Epoch: 30 | train_loss: 0.148519 | train_acc: 0.924231 | val_loss: 0.453439 | val_acc: 0.831845 | lr: 0.00000001\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "training_results = train(model, train_loader, val_loader, optimizer, loss_function, epochs=30, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
